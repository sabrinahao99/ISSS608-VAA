---
title: "Take-home Exercise 1"
author: "Zhiyi Hao"
date: "2026/02/07"
format:
  html:
    backgroundcolor: white
    fontcolor: black
    linkcolor: blue
---

# 1.Overview

## 1.1 Background

The survey conducted by the Vietnam National University of Education (VNU), is aimed to collect data on student learning outcomes (measured by GPA) and included contextual information that could influence academic performance, such as family background, personal habits, and school environment factors.

Previous research (particularly in the Vietnamese context) has indicated that student motivation, instructor competence, and accessibility of learning facilities are key drivers of improved academic performance. This dataset allows educators to test these hypotheses, shifting their focus from simply monitoring grades to understanding how family socioeconomic status (e.g., parents' education and employment status) and school support systems interact to influence learning outcomes.

The data used in this exercise was collected from March to June 2023, and the questionnaire primarily covered two categories: Group A (Students' demographic characteristics) and Group B (student learning perceptions during university).

## 1.2 The Data

The data for this exercise is the Student questionnaire data(database paper) file in .csv format, retrieved from [Mendeley Data Repository](https://data.mendeley.com/datasets/23ppcdbmhc/1). ![](image1.png)

Supplementary materials used in conjunction with dataset:

-   [Codebook](https://data.mendeley.com/datasets/23ppcdbmhc/1)

## 1.3 The Task

Based on the 2023 VNU Student Questionnaire Dataset, this post aims to reveal followings by using appropriate **Exploratory Data Analysis (EDA) methods** and **the ggplot2 package**

-   the distribution of academic performance (GPA) among students at Vietnam National University across different grades and genders;

-   the interrelationships between these academic performances and family socioeconomic status (parental education level), personal behavioral habits (time spent studying and social media), and perceived school support.

# 2.Loading Packages

In this exercise, several R packages will be used. They are listed in the table below:

| Library | Description |
|:-----------------------------------|:-----------------------------------|
| [**tidyverse**](https://www.tidyverse.org/) | A collection of core packages used for data preparation and wrangling |
| [**knitr**](https://yihui.org/knitr/) | For dynamic report generation |
| [**patchwork**](https://patchwork.data-imaginist.com/) | For preparing composite figure created using ggplot2 |
| [**ggthemes**](https://jrnold.github.io/ggthemes/) | Extra themes, geoms, and scales for ggplot2 |
| [**ggridges**](https://wilkelab.org/ggridges/) | For ridgeline plots that create the impression of a mountain range.can be useful for visualising changes in distributions over time |
| [**ggdist**](https://mjskay.github.io/ggdist/) | For visualisations of distributions and uncertainty. Key package for creating Raincloud Plots |
| [**treemapify**](https://wilkox.org/treemapify/) | Provides 'ggplot2' geoms for drawing Treemaps, used for visualising hierarchical demographic data |
| [**ggrepel**](https://ggrepel.slowkow.com/) | Provides geoms for ggplot2 to repel overlapping text labels. Essential for annotating outliers |
| [**ggcorrplot**](http://www.sthda.com/english/wiki/ggcorrplot) | For visualizing a correlation matrix using ggplot2 |
| [**ggnewscale**](https://eliocamp.github.io/ggnewscale/) | For defining scales in ggplot2 |
| [**ggtext**](https://wilkelab.org/ggtext/) | Supports improved text rendering for ggplot2. |
| [**ggpubr**](https://rpkgs.datanovia.com/ggpubr/) | Provides some easy-to-use functions for creating and customizing ‘ggplot2’- based publication ready plots |

The following code chunk uses `p_load()` of [**pacman**](https://rpubs.com/akshaypatankar/594834) package to check if tidyverse packages are installed in the computer. If they are, the libraries will be called into R.

```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
pacman::p_load(tidyverse, knitr, patchwork, 
               ggthemes, ggridges, ggdist, 
               treemapify, ggrepel, ggcorrplot,
               ggnewscale,ggtext,ggpubr)
```

# 3.Data Import

In sections 3 and 4, I will explain all the steps of data processing — from reading the raw data to generating the final dataset. I will use the tidyverse package, specifically dplyr for data cleaning and readr for reading the data.

## 3.1 Importing .xlsx file

The following code block uses the `read_excel()` function from the `readxl` package to import the original dataset. This data originates from the file `Database paper.xlsx`.

```{r}
library(readxl)
raw_dataset <- read_excel("data/Database paper.xlsx")
glimpse(raw_dataset)
```

This dataset contains 2,170 observations (rows) and 22 variables (columns). Each observation corresponds to a participating student or alumnus, and the variables cover students' demographic characteristics, family background, and perceptions of the school environment.

Our raw data was provided as an Excel file (.xlsx). After importing the data into R, we saved it as a copy in RDS format (.rds) for faster data reloading and improved result reproducibility.

RDS files are generally more space-efficient than text-based formats such as CSV, and more importantly, they preserve R data types (such as factors/ordinal factors defined later), thus avoiding the need to redefine column types every time the dataset is re-imported.

```{r}
write_rds(raw_dataset, "data/raw_dataset.rds")
raw_dataset <- read_rds("data/raw_dataset.rds")
```

## 3.2 Duplicates Check

To ensure data quality, we need to check there are duplicated records in the dataset.

```{r}
duplicate_count <- raw_dataset %>% 
  duplicated() %>% 
  sum()
paste("Number of duplicate rows:", duplicate_count)
```

```{r}
sum(duplicated(raw_dataset))
nrow(raw_dataset)
```

We can find the duplicated record is 226 out of the raw data sample of 2170, however, there is no distinct record like ID in our dataset, we can't treat all duplicated data as the true duplicated ones. Since repeated response patterns may represent the true frequency in the population, duplication based solely on identical values in the survey variables could distort the empirical distribution and introduce bias into subsequent analyses. Therefore, we retained all records.

# **4.Data Wrangling**

Upon observation, I found that the raw data was rarely usable directly for analysis. For example, many variables in this dataset used numerical encoding (e.g., "1/2" instead of "Male/Female"), and some column names contained spelling errors (e.g., "Time_SocicalMedia", "InfuenceF_Friends"). Therefore, by following the official codebook, I transformed the raw data into a clean dataset suitable for analysis using the following steps.

4.1 Variable Selection
I select some useful variables required for subsequent EDA process and ensure the raw column names are consistently referenced.
4.2 Handling missing values

4.3 Recoding Categorical Variables

4.3.1 Coding schema (Data Dictionary)

4.3.2 Apply recoding (based on codebook)

4.4 Data Type Conversion

4.5 Variable Creation

4.5.1 Creating numeric GPA (for continuous visualisations)

4.5.2 Final checks

We verify that categorical variables are correctly stored as factors / ordered factors.

# 5.Univariate Analysis

# 6.Bivariate & Multivariate Analysis

# 7.Summary

# 8.References
