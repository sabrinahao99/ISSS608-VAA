[
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html",
    "title": "Take-home Exercise 1final",
    "section": "",
    "text": "Nowadays, researchers have long been dedicated to researching the factors influencing student academic performance. The dataset used in this analysis comes from VNU University of Education and aims to comprehensively explore the deep relationship between various real-world factors and student academic achievement (GPA).\nThe survey was conducted from March to June 2023, collecting 2,170 responses from students. The data primarily covers two core dimensions: 1) students‚Äô demographic characteristics and lifestyle habits (e.g., Academic Year, parental educational background, allocation of their time); 2) students‚Äô learning perceptions at university(e.g., feelings of teaching quality, facilities, influence from friends etc.).\nThrough exploratory data analysis (EDA) of these different types of data, we hope to identify the key indicators may affectiing students‚Äô GPA and find relationships. This will not only provide school administrators with objective feedback but also help teachers and academic advisors better understand the factors influencing student performance, and also enabling them to take measures if needed to improve overall teaching effectiveness.\n\n\n\nThe data for this exercise is the Student questionnaire data(database paper) file in .xlsx format, retrieved from Mendeley Data Repository. \nSupplementary materials used in conjunction with dataset:\n\nCodebook\n\n\n\n\nBased on the 2023 VNU Student Questionnaire Dataset, this post aims to reveal followings by using appropriate Exploratory Data Analysis (EDA) methods and the ggplot2 package\n\nthe distribution of academic performance (GPA)across different year students at Vietnam National University\nthe relationships between these academic performances and family socioeconomic status (parental education level), personal behavioral habits (time spent studying and social media), and perception varaibles."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#background",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#background",
    "title": "Take-home Exercise 1final",
    "section": "",
    "text": "Nowadays, researchers have long been dedicated to researching the factors influencing student academic performance. The dataset used in this analysis comes from VNU University of Education and aims to comprehensively explore the deep relationship between various real-world factors and student academic achievement (GPA).\nThe survey was conducted from March to June 2023, collecting 2,170 responses from students. The data primarily covers two core dimensions: 1) students‚Äô demographic characteristics and lifestyle habits (e.g., Academic Year, parental educational background, allocation of their time); 2) students‚Äô learning perceptions at university(e.g., feelings of teaching quality, facilities, influence from friends etc.).\nThrough exploratory data analysis (EDA) of these different types of data, we hope to identify the key indicators may affectiing students‚Äô GPA and find relationships. This will not only provide school administrators with objective feedback but also help teachers and academic advisors better understand the factors influencing student performance, and also enabling them to take measures if needed to improve overall teaching effectiveness."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#the-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#the-data",
    "title": "Take-home Exercise 1final",
    "section": "",
    "text": "The data for this exercise is the Student questionnaire data(database paper) file in .xlsx format, retrieved from Mendeley Data Repository. \nSupplementary materials used in conjunction with dataset:\n\nCodebook"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#the-task",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#the-task",
    "title": "Take-home Exercise 1final",
    "section": "",
    "text": "Based on the 2023 VNU Student Questionnaire Dataset, this post aims to reveal followings by using appropriate Exploratory Data Analysis (EDA) methods and the ggplot2 package\n\nthe distribution of academic performance (GPA)across different year students at Vietnam National University\nthe relationships between these academic performances and family socioeconomic status (parental education level), personal behavioral habits (time spent studying and social media), and perception varaibles."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#importing-.xlsx-file",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#importing-.xlsx-file",
    "title": "Take-home Exercise 1final",
    "section": "3.1 Importing .xlsx file",
    "text": "3.1 Importing .xlsx file\nThe following code block uses the read_excel() function from the readxl package to import the original dataset. This data originates from the file Database paper.xlsx.\n\n\nCode\nlibrary(readxl)\nraw_dataset &lt;- read_excel(\"data/Database paper.xlsx\")\nraw_dataset\n\n\n# A tibble: 2,170 √ó 22\n    Year Gender Policy_Stu Minority_Stu Poor_Stu Father_Edu Mother_Edu\n   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1     5      2          2            2        2          4          4\n 2     5      1          2            2        2          3          3\n 3     5      2          2            2        2          4          4\n 4     5      2          2            2        2          5          4\n 5     5      1          1            2        2          2          3\n 6     5      2          2            2        2          5          5\n 7     5      2          2            2        2          6          5\n 8     5      2          2            2        2          5          4\n 9     5      2          2            2        2          5          5\n10     5      2          2            2        2          5          4\n# ‚Ñπ 2,160 more rows\n# ‚Ñπ 15 more variables: Father_Occupation &lt;dbl&gt;, Mother_Occupation &lt;dbl&gt;,\n#   Time_Friends &lt;dbl&gt;, Time_SocicalMedia &lt;dbl&gt;, Time_Studying &lt;dbl&gt;,\n#   GPA &lt;dbl&gt;, Adapt_Learning_Uni &lt;dbl&gt;, Study_Methods &lt;dbl&gt;,\n#   SupportOf_Uni &lt;dbl&gt;, SupportOf_Lec &lt;dbl&gt;, Facilitie_Uni &lt;dbl&gt;,\n#   Quality_Lecturer &lt;dbl&gt;, TrainingCurriculum &lt;dbl&gt;, Competitive_Class &lt;dbl&gt;,\n#   InfuenceF_Friends &lt;dbl&gt;\n\n\nThis dataset contains 2,170 observations (rows) and 22 variables (columns). Each observation corresponds to a participating student or alumnus, and the variables cover students‚Äô demographic characteristics, family background, and perceptions of the school environment."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#duplicates-check",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#duplicates-check",
    "title": "Take-home Exercise 1final",
    "section": "3.2 Duplicates Check",
    "text": "3.2 Duplicates Check\nTo ensure data quality, we need to check there are duplicated records in the dataset.\n\n\nCode\nsum(duplicated(raw_dataset))\n\n\n[1] 226\n\n\nCode\nnrow(raw_dataset)\n\n\n[1] 2170\n\n\nWe can find the duplicated record is 226 out of the raw data sample of 2170, however, there is no distinct record like ID in our dataset, we can‚Äôt treat all duplicated data as the true duplicated ones. Since repeated response patterns may represent the true frequency in the population, duplication based solely on identical values in the survey variables could distort the empirical distribution and introduce bias into subsequent analyses. Therefore, we retained all records."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#variable-selection",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#variable-selection",
    "title": "Take-home Exercise 1final",
    "section": "4.1 Variable Selection",
    "text": "4.1 Variable Selection\nFirst, I select some useful variables required for EDA process and ensure the raw column names are consistently referenced. To ensure that the variables for subsequent visualization are cleaned, we should eliminate noise(drop useless variables). For the selection of direct variables, I chose to retain only those directly related to demographics, learning habits, and personal perceptions. In this step, I specifically excluded the following variables for the following reasons:\n\nGender(A.2): To focus the analysis on universally applicable learning strategies and perceptual dimensions, gender was considered noise and therefore excluded.\nSocioeconomic indicators (A.3, A.4, A.5): I excluded ‚Äúpolicy support,‚Äù ‚Äúminority status,‚Äù and ‚Äúpoverty status.‚Äù This analysis emphasizes individual student behaviors more than macro-level ‚Äúsocioeconomic background factors.‚Äù\nParental Occupation indicators (A.8, A.9): I selected ‚Äúparents‚Äô education level‚Äù and excluded ‚Äúparents‚Äô occupation.‚Äù In educational research, education level is generally considered a more stable and explanatory indicator of family cultural capital.\n\nSince most variables in the original dataset were numerical, I recoded them into meaningful and easily interpretable category labels according to the official coding manual. This step involved converting the raw data into ordinal factors (e.g., converting ‚Äú1, 2, 3‚Äù into ‚Äúnone, moderate, very‚Äù), thus ensuring that subsequent visualizations and statistical tests were ordinal and valid.\n\n\nCode\nfinal_ggplot &lt;- raw_dataset\n\nfinal_ggplot$Year_Label &lt;- factor(final_ggplot$Year, levels = 1:5, \n    labels = c(\"First-year\", \"Second-year\", \"Third-year\", \"Fourth-year\", \"Graduated\"))\n\nfinal_ggplot$Father_Edu_Label &lt;- factor(final_ggplot$Father_Edu, levels = 1:6, \n    labels = c(\"Primary\", \"Secondary\", \"High School\", \"College\", \"University/Grad\", \"Other\"))\n\nfinal_ggplot$Mother_Edu_Label &lt;- factor(final_ggplot$Mother_Edu, levels = 1:6, \n    labels = c(\"Primary\", \"Secondary\", \"High School\", \"College\", \"University/Grad\", \"Other\"))\n\nfinal_ggplot$Friends_Label &lt;- ordered(final_ggplot$Time_Friends, levels = 1:5, \n    labels = c(\"&lt;1h\", \"1-2h\", \"2-3h\", \"3-4h\", \"&gt;=4h\"))\n\nfinal_ggplot$Social_Media_Label &lt;- ordered(final_ggplot$Time_SocicalMedia, levels = 1:5, \n    labels = c(\"&lt;1h\", \"1-2h\", \"2-3h\", \"3-4h\", \"&gt;=4h\"))\n\nfinal_ggplot$Study_Time_Label &lt;- ordered(final_ggplot$Time_Studying, levels = 1:5, \n    labels = c(\"&lt;2h\", \"2-4h\", \"4-6h\", \"6-8h\", \"&gt;=8h\"))\n\nfinal_ggplot$GPA_Label &lt;- ordered(final_ggplot$GPA, levels = 1:5, \n    labels = c(\"&lt;2.0 (Poor)\", \"2.0‚Äì2.5 (Avg)\", \"2.5‚Äì3.2 (Fair)\", \"3.2‚Äì3.6 (Good)\", \"&gt;=3.6 (Excl)\"))\n\ngpa_map &lt;- c(1.50, 2.25, 2.85, 3.40, 3.85)\nfinal_ggplot$GPA_Num &lt;- gpa_map[as.numeric(as.character(final_ggplot$GPA))]\n\nfinal_ggplot$Adapt_Learning_Uni &lt;- ordered(final_ggplot$Adapt_Learning_Uni, levels = 1:5, \n                                           labels = c(\"Not at all\", \"Little\", \"Moderate\", \"Quite\", \"Very\"))\n\nfinal_ggplot$Study_Methods      &lt;- ordered(final_ggplot$Study_Methods, levels = 1:5, \n                                           labels = c(\"Not at all\", \"Little\", \"Moderate\", \"Quite\", \"Very\"))\n\nfinal_ggplot$SupportOf_Uni      &lt;- ordered(final_ggplot$SupportOf_Uni, levels = 1:5, \n                                           labels = c(\"Not at all\", \"Little\", \"Moderate\", \"Quite\", \"Very\"))\n\nfinal_ggplot$SupportOf_Lec      &lt;- ordered(final_ggplot$SupportOf_Lec, levels = 1:5, \n                                           labels = c(\"Not at all\", \"Little\", \"Moderate\", \"Quite\", \"Very\"))\n\nfinal_ggplot$Facilitie_Uni      &lt;- ordered(final_ggplot$Facilitie_Uni, levels = 1:5, \n                                           labels = c(\"Not at all\", \"Little\", \"Moderate\", \"Quite\", \"Very\"))\n\nfinal_ggplot$Quality_Lecturer   &lt;- ordered(final_ggplot$Quality_Lecturer, levels = 1:5, \n                                           labels = c(\"Not at all\", \"Little\", \"Moderate\", \"Quite\", \"Very\"))\n\nfinal_ggplot$TrainingCurriculum &lt;- ordered(final_ggplot$TrainingCurriculum, levels = 1:5, \n                                           labels = c(\"Not at all\", \"Little\", \"Moderate\", \"Quite\", \"Very\"))\n\nfinal_ggplot$Competitive_Class  &lt;- ordered(final_ggplot$Competitive_Class, levels = 1:5, \n                                           labels = c(\"Not at all\", \"Little\", \"Moderate\", \"Quite\", \"Very\"))\n\nfinal_ggplot$InfuenceF_Friends  &lt;- ordered(final_ggplot$InfuenceF_Friends, levels = 1:5, \n                                           labels = c(\"Not at all\", \"Little\", \"Moderate\", \"Quite\", \"Very\"))\n#mapping GPA into numerical form GPA_Num\nfinal_ggplot$Study_Methods_Raw &lt;- raw_dataset$Study_Methods\nfinal_ggplot$GPA_Raw &lt;- raw_dataset$GPA\ngpa_map &lt;- c(1.50, 2.25, 2.85, 3.40, 3.85)\nfinal_ggplot$GPA_Num &lt;- gpa_map[as.numeric(as.character(final_ggplot$GPA_Raw))]\nfinal_ggplot$Study_Methods_Fix &lt;- factor(final_ggplot$Study_Methods_Raw, \n                                         levels = 1:5, \n                                         labels = c(\"Not at all\", \"Little\", \"Moderate\", \"Quite\", \"Very\"))\n\n\nNote:Under is &lt;, Over is &gt;=, use symbols simplifies the final presentation."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#final-checks-for-dataset-quality",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#final-checks-for-dataset-quality",
    "title": "Take-home Exercise 1final",
    "section": "4.2 Final checks for dataset quality",
    "text": "4.2 Final checks for dataset quality\n\n\nCode\n#check if GPA_Num exists and missing value\nsummary(final_ggplot$GPA_Num)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.500   2.850   2.850   2.999   3.400   3.850 \n\n\nCode\nanyNA(final_ggplot)\n\n\n[1] FALSE\n\n\nThe final checks confirmed that the dataset contained no missing values and no duplicate rows. Furthermore, all ordered variables were presented in the order of the official codebook.\nAfter doing these two checks, we can make sure the data quality is good(no missing value, normal distribution,factor order is correct) and it is applicable for EDA process."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#viz1-academic-performance-exhibits-high-median-stability-but-increasing-divergence-in-later-study-years",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#viz1-academic-performance-exhibits-high-median-stability-but-increasing-divergence-in-later-study-years",
    "title": "Take-home Exercise 1final",
    "section": "Viz1: Academic performance exhibits high median stability but increasing divergence in later study years",
    "text": "Viz1: Academic performance exhibits high median stability but increasing divergence in later study years\nTo explore the evolution of GPA distribution across different grade levels, I first cleaned the raw data. I mapped the categorical variable GPA to numerical midpoints (e.g., mapping grade 1 to 1.50 and grade 5 to 3.85), making GPA an analyzable numerical score for easy visualization. Next, I used the ggridges package to create a ridge plot to show the probability density distribution of grades across grade levels. Then, I used stat_density_ridges to automatically calculate quantiles. The extreme 2.5% regions at both ends of the distribution are highlighted in red and blue in the plot. This approach not only visually presents the central tendency of grades across grade levels but also effectively identifies the proportion of students with abnormally high or low GPAs.\n\n\nCode\n#stats(quantile)\nggplot(final_ggplot, \n       aes(x = GPA_Num, \n           y = Year_Label, \n           fill = factor(stat(quantile)))) + \n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n  ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  \n  theme_ridges() +\n labs(\n    title = \"Distribution of Student GPA Scores by Academic Year\",\n    subtitle = \"Academic performance exhibits high median stability but increasing divergence in later study years\",\n    x = \"GPA Score (Mapping GPA category)\",\n    y = \"Academic Year\"\n  )+\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0),\n    plot.subtitle = element_text(size = 10, color = \"gray30\", margin = margin(b = 10)),\n    axis.title.x = element_text(size = 11, face = \"plain\"),\n    axis.title.y = element_text(size = 11, face = \"plain\"),\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteüò∫ Observations\n\n\n\n\nOverall, the GPA of students across all grades is around 3.0 (between Fair and Good), showing a significant peak. This indicates that the school‚Äôs students have very stable academic performance, with no extremely low scores, and the overall teaching quality is good.\nAs students progress through the years, they should be more adapted to university life, but the red zone on the left remains stable across all grades, especially in the third year. The reason for such a high proportion may need to be explored in terms of student perception or other aspects, such as their level of adaptation.\nTowards graduation, the density of the blue zone on the right increases slightly, and the curve slopes more pronounced to the right. This may be because students nearing graduation have found a better study method, thus learning better and faster, resulting in a higher probability of having a high GPA."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#viz2-higher-parental-education-significantly-boosts-the-probability-of-achieving-a-high-gpa",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#viz2-higher-parental-education-significantly-boosts-the-probability-of-achieving-a-high-gpa",
    "title": "Take-home Exercise 1final",
    "section": "Viz2: Higher parental education significantly boosts the probability of achieving a high GPA",
    "text": "Viz2: Higher parental education significantly boosts the probability of achieving a high GPA\nI first defined a high GPA (same as year). A student‚Äôs GPA was recorded as TRUE if it was in the range of ‚Äú3.2-3.6 (Good)‚Äù or ‚Äú‚â•3.6 (Excellent),‚Äù and as FALSE otherwise. This binary variable fairly compares the relationship between parental education level and GPA. Next, I summarized the data by Father_Edu_Label and Mother_Edu_Label, calculating the sample size and high GPA rate for each group. Then, to determine the baseline for the funnel plot, I calculated a weighted average of these rates with a weight of \\(1/se^2\\) to ensure that groups with larger sample sizes and more stable estimates have a greater impact on the overall mean. Next, I generated 95% (dashed line) and 99.9% (solid line) confidence intervals to define the expected range of variation. Finally, I constructed the funnel plot using ggplot2, applied geom_text to add labels for each education group, and ensured readability by managing label overlap.\nFather‚Äôs education background\n\n\nCode\n# compute high GPA rate\nhigh_gpa &lt;- final_ggplot$GPA_Label %in% c(\"3.2‚Äì3.6 (Good)\", \"&gt;=3.6 (Excl)\")\nn_tab &lt;- table(final_ggplot$Father_Edu_Label)\nrate_tab &lt;- tapply(high_gpa, final_ggplot$Father_Edu_Label, mean, na.rm = TRUE)\ndf &lt;- data.frame(\n  Group = names(n_tab),\n  Positive = as.numeric(n_tab),\n  rate = as.numeric(rate_tab)\n)\n\n# exclude the small size group\ndf &lt;- df[df$Positive &gt;= 5, ]\ndf$rate.se &lt;- sqrt((df$rate * (1 - df$rate)) / df$Positive)\n#compute fit.mean\nfit.mean &lt;- weighted.mean(df$rate, 1 / df$rate.se^2)\n# Calculate lower and upper limits for 95% and 99.9% CI\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95  &lt;- fit.mean - 1.96 * sqrt((fit.mean * (1 - fit.mean)) / number.seq)\nnumber.ul95  &lt;- fit.mean + 1.96 * sqrt((fit.mean * (1 - fit.mean)) / number.seq)\nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean * (1 - fit.mean)) / number.seq)\nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean * (1 - fit.mean)) / number.seq)\ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, number.ul999, number.seq, fit.mean)\n# plot\np1 &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(alpha = 0.4, color = \"red\") +\n  geom_text(aes(label = Group), vjust = -1, size = 3, check_overlap = TRUE) +\n  geom_line(data = dfCI, aes(x = number.seq, y = number.ll95), \n            linewidth = 0.4, colour = \"grey40\", linetype = \"dashed\") +\n  geom_line(data = dfCI, aes(x = number.seq, y = number.ul95), \n            linewidth = 0.4, colour = \"grey40\", linetype = \"dashed\") +\n  geom_line(data = dfCI, aes(x = number.seq, y = number.ll999), \n            linewidth = 0.4, colour = \"grey40\") +\n  geom_line(data = dfCI, aes(x = number.seq, y = number.ul999), \n            linewidth = 0.4, colour = \"grey40\") +\n  geom_hline(data = dfCI, aes(yintercept = fit.mean), \n             linewidth = 0.4, colour = \"grey40\") +\n  coord_cartesian(ylim = c(0, 1)) +\n  annotate(\"text\", x = max(df$Positive)*0.95, y = fit.mean + 0.05, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = max(df$Positive)*0.95, y = fit.mean + 0.15, label = \"99.9%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"High GPA Rate by Father's Educational Background\") +\n  xlab(\"Sample Size\") + \n  ylab(\"High GPA Rate (&gt;= 3.2)\") +\n  theme_light() +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        legend.position = \"none\") \n\np1\n\n\n\n\n\n\n\n\n\nMother‚Äôs education background\n\n\nCode\n# compute \nhigh_gpa &lt;- final_ggplot$GPA_Label %in% c(\"3.2‚Äì3.6 (Good)\", \"&gt;=3.6 (Excl)\")\nn_tab_m &lt;- table(final_ggplot$Mother_Edu_Label)\nrate_tab_m &lt;- tapply(high_gpa, final_ggplot$Mother_Edu_Label, mean, na.rm = TRUE)\n\ndf_m &lt;- data.frame(\n  Group = names(n_tab_m),\n  Positive = as.numeric(n_tab_m),\n  rate = as.numeric(rate_tab_m)\n)\ndf_m &lt;- df_m[df_m$Positive &gt;= 5, ]\ndf_m$rate.se &lt;- sqrt((df_m$rate * (1 - df_m$rate)) / df_m$Positive)\n#compute fit.mean\nfit.mean_m &lt;- weighted.mean(df_m$rate, 1 / df_m$rate.se^2)\n# Calculate lower and upper limits for 95% and 99.9% CI\nnumber.seq &lt;- seq(1, max(df_m$Positive), 1)\nnumber.ll95  &lt;- fit.mean_m - 1.96 * sqrt((fit.mean_m * (1 - fit.mean_m)) / number.seq)\nnumber.ul95  &lt;- fit.mean_m + 1.96 * sqrt((fit.mean_m * (1 - fit.mean_m)) / number.seq)\nnumber.ll999 &lt;- fit.mean_m - 3.29 * sqrt((fit.mean_m * (1 - fit.mean_m)) / number.seq)\nnumber.ul999 &lt;- fit.mean_m + 3.29 * sqrt((fit.mean_m * (1 - fit.mean_m)) / number.seq)\ndfCI_m &lt;- data.frame(number.ll95, number.ul95, number.ll999, number.ul999, number.seq, fit.mean_m)\n#plot\np2 &lt;- ggplot(df_m, aes(x = Positive, y = rate)) +\n  geom_point(alpha = 0.4, color = \"red\") +\n  geom_text(aes(label = Group), vjust = -1, size = 3, check_overlap = TRUE) +\n  geom_line(data = dfCI_m, aes(x = number.seq, y = number.ll95), \n            linewidth = 0.4, colour = \"grey40\", linetype = \"dashed\") +\n  geom_line(data = dfCI_m, aes(x = number.seq, y = number.ul95), \n            linewidth = 0.4, colour = \"grey40\", linetype = \"dashed\") +\n  geom_line(data = dfCI_m, aes(x = number.seq, y = number.ll999), \n            linewidth = 0.4, colour = \"grey40\") +\n  geom_line(data = dfCI_m, aes(x = number.seq, y = number.ul999), \n            linewidth = 0.4, colour = \"grey40\") +\n  geom_hline(data = dfCI_m, aes(yintercept = fit.mean_m), \n             linewidth = 0.4, colour = \"grey40\") +\n  coord_cartesian(ylim = c(0, 1)) +\n  annotate(\"text\", x = max(df_m$Positive)*0.95, y = fit.mean_m + 0.05, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = max(df_m$Positive)*0.95, y = fit.mean_m + 0.15, label = \"99.9%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"High GPA Rate by Mother's Educational Background\") +\n  xlab(\"Sample Size (n)\") + \n  ylab(\"High GPA Rate (&gt;= 3.2)\") +\n  theme_light() +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        legend.position = \"none\") \n\np2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteüò∫ Observations\n\n\n\n\nBoth graphs show that the points for the University/Graduate education group are generally higher, while those for Primary School/Secondary School are lower. This suggests strongly a positive correlation between parental education level and the probability of their child achieving a high GPA.\nFurthermore, most points fall within the 95%/99% threshold, indicating no significant differences within the sample size and normal levels compared to the overall average.\nFrom an overall perspective, therefore, we can conclude that while there is a trend where higher parental education levels generally correlate with higher GPA rates."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#viz3-mothers-education-shows-a-more-consistent-impact-on-student-gpa",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#viz3-mothers-education-shows-a-more-consistent-impact-on-student-gpa",
    "title": "Take-home Exercise 1final",
    "section": "Viz3: Mother‚Äôs education shows a more consistent impact on student GPA",
    "text": "Viz3: Mother‚Äôs education shows a more consistent impact on student GPA\nFirst, we need to process the fields ‚Äì defining High GPA. This requires converting the original GPA_Cat into a binary variable, where high rate means: when a student‚Äôs GPA falls within ‚Äú3.2‚Äì&lt;3.6 (Good)‚Äù or ‚Äú‚â•3.6 (Excellent),‚Äù it is recorded as TRUE; otherwise, it is FALSE. This can be used to compare the high GPA performance of different groups. Next, we group by parent‚Äôs education level and calculate the High GPA rate. First, we group separately for Father_Edu / Mother_Edu, calculating n (sample size) and rate. Then, we calculate the overall mean, using weights of 1 / rate.se^2 to give higher weights to groups with larger sample sizes and more stable estimates. Then, we generate the 95% / 99% confidence limits for the funnel. Finally, we use the above output to generate the plot. In addition to ggplot, ggrepel is also used to prevent overlap.\n\n\nCode\np_fa_tail &lt;- ggplot(final_ggplot, \n                   aes(x = GPA_Num, y = Father_Edu_Label, \n                       fill = 0.5 - abs(0.5 - stat(ecdf)))) + \n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail Prob.\", \n                       direction = -1,\n                       option = \"mako\") + \n  theme_ridges() +\n  labs(title = \"Father's Education\", \n       x = \"GPA Score\", y = \"Education Level\") +\n  theme(legend.position = \"none\")\np_mo_tail &lt;- ggplot(final_ggplot, \n                   aes(x = GPA_Num, y = Mother_Edu_Label, \n                       fill = 0.5 - abs(0.5 - stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail Prob.\", \n                       direction = -1,\n                       option = \"mako\") +\n  theme_ridges() +\n  labs(title = \"Mother's Education\", \n       x = \"GPA Score\", y = \"\") +\n  theme(legend.position = \"right\")\nall_plot_tail &lt;- p_fa_tail + p_mo_tail + \n  plot_annotation(\n    title = \"Student Probability Distribution of GPA by Parental Education\",\n    subtitle = \"Comparison of GPA distribution shifts, highlighting the narrower variance and stronger core concentration in mother's education groups\",\n    theme = theme(plot.title = element_text(size = 16, face = \"bold\"))\n  )\n\nall_plot_tail\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteüò∫ Observations\n\n\n\n\nFirst, we‚Äôve just observed from the tunnel plot that the darkest areas of highest student density show a continuous rightward shift as parental education levels increase. This further confirms our assessment that higher parental education levels are more conducive to cultivating top students and also help prevent children from achieving excessively low grades.\nComparing the two plots, the darkest core area for mothers appears more compact and sharper than for fathers. Furthermore, the high density near the median suggests that mothers‚Äô education levels may play a more significant role in students‚Äô GPAs than fathers‚Äô."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#viz4-high-gpa-students-spent-more-time-on-study-while-spent-less-on-social-media",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#viz4-high-gpa-students-spent-more-time-on-study-while-spent-less-on-social-media",
    "title": "Take-home Exercise 1final",
    "section": "Viz4: High GPA students spent more time on study while spent less on social media",
    "text": "Viz4: High GPA students spent more time on study while spent less on social media\nHere, I use a ternary plot to show the proportions of students spending time on three categories of daily activities (studying/social media/going out with friends). I also color-code points according to different GPA categories to reflect the distribution differences among different GPA categories. The implementation process is as follows: First, I convert the grouping intervals of the three time variables into midpoints for easier comparison, based on the official codebook. For example, social media and going out with friends are 0.5/1.5/2.5/3.5/4.5, while study time‚Äîless than 2 hours of study time‚Äîis 1, and so on, 3/5/7/9. Then, I obtain three new variables, sm_new, fri_new, and st_new, from the defined midpoints. By calculating the total time (total), and dividing the three new variables by total, I obtain their respective proportions, sm_p, fri_p, and st_p.¬†Finally, I convert GPA_Cat into a factor, use ggtern() to draw the plot, and color-code it according to GPA category to observe the clustering trend of different GPA groups in the three types of time allocation.\n\n\nCode\nmidp_sm_fri &lt;- c(0.5, 1.5, 2.5, 3.5, 4.5) \nmidp_study  &lt;- c(1, 3, 5, 7, 9)\ntime_tri_mutated &lt;- final_ggplot %&gt;%\n  mutate(\n    # turn categorical variable into midpoint value\n    sm_val = midp_sm_fri[as.integer(Time_SocicalMedia)],\n    fri_val = midp_sm_fri[as.integer(Time_Friends)],\n    st_val  = midp_study[as.integer(Time_Studying)],\n    TOTAL = sm_val + fri_val + st_val\n  ) %&gt;%\n  filter(TOTAL &gt; 0) \n#plot \nggtern(data = time_tri_mutated, aes(x = st_val, y = sm_val, z = fri_val, colour = GPA_Label)) +\n  geom_point(alpha = 0.6, size = 2) +\n  labs(\n    title = \"High GPA students spent more time on study while spent less on social media\",\n    subtitle = \"Ternary plot shows the trade-off between Study, Social Media, and Friends\",\n    colour = \"GPA Category\"\n  ) +\n    theme_rgbw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteüò∫ Observations\n\n\n\n\nAs can be seen from the distribution of the data points, most points are clearly clustered near the Social Media area, while there is almost no clustering near the Study and Outings locations. This indicates that most students in the sample spend their time on social media, with relatively less time allocated to studying and outings.\nAt the GPA category level, the distribution of points within each subcategory has a high degree of overlap, and the hypothetical scenario of ‚Äúhigh GPA being significantly closer to the learning peak‚Äù does not occur. Here we can conclude that judging by the proportion of time spent near the midpoint of the interval, the differences in GPA between groups may be weak. Further statistical tests are needed to confirm this (testing each of the three variables against the GPA Category will yield a more rigorous conclusion).\n\n\n\n\n\nViz5: Does high GPA students spent less time on social media and more on study?\nFirst, I use ggbetweenstats() to test and display the differences between the three time variables and GPA groups. Since the upper limits of the scales for the three time variables are inconsistent (the highest for study time is ‚Äú‚â•8 hours‚Äù, while the highest for others is ‚Äú&gt;4 hours‚Äù), to ensure comparability, I converted each category to an interval midpoint according to the official codebook, thus unifying the unit of measurement to ‚Äúhours‚Äù. After mapping the midpoints, I then generated three numerical variables‚ÄîStudy_hours, Media_hours, and Friends_hours‚Äîbased on the selected fields, and performed tests with GPA_Cat as the grouping variable (x) and time as y.\nFor statistical testing, considering that the distribution of the time variables may be non-normal and that the sample sizes between groups are unbalanced, I still used the Kruskal-Wallis nonparametric test to attempt to test the differences in time allocation among different GPA groups.\n\n\nCode\n# use mutate\ndf_time_analysis &lt;- final_ggplot %&gt;%\n  mutate(\n    st_hours  = midp_study[as.integer(Time_Studying)],\n    sm_hours  = midp_sm_fri[as.integer(Time_SocicalMedia)],\n    fri_hours = midp_sm_fri[as.integer(Time_Friends)],\n    TOTAL     = st_hours + sm_hours + fri_hours\n  ) %&gt;%\n  filter(TOTAL &gt; 0)\n# use ggbetweenstats \nviz_study &lt;- ggbetweenstats(\n  data = df_time_analysis,\n  x = GPA_Label,\n  y = st_hours,\n  type = \"np\", #non-parametric\n  messages = FALSE\n) + labs(\n  title = \"Study hours across GPA categories\",\n  x = \"GPA Category\", \n  y = \"Study Time (hours)\"\n)\n\nviz_media &lt;- ggbetweenstats(\n  data = df_time_analysis,\n  x = GPA_Label,\n  y = sm_hours,\n  type = \"np\",\n  messages = FALSE\n) + labs(\n  title = \"Social media time across GPA categories\",\n  x = \"GPA Category\", \n  y = \"Social Media Time (hours)\"\n)\n\nviz_friends &lt;- ggbetweenstats(\n  data = df_time_analysis,\n  x = GPA_Label,\n  y = fri_hours,\n  type = \"np\",\n  messages = FALSE\n) + labs(\n  title = \"Time spent with friends across GPA categories\",\n  x = \"GPA Category\", \n  y = \"Friends Time (hours)\"\n)\n#patchwork\n(viz_study) / (viz_media | viz_friends)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteüò∫ Observations\n\n\n\nTo compare students‚Äô time-use distributions across GPA groups, we applied the Kruskal‚ÄìWallis (non-parametric) test. All three time variables were statistically significant across GPA categories.\n‚Ä¢ Study time: Median study time was 9 hours for every GPA group, mainly because many students fall into the ‚â•8 hours band. The clearest difference is that the lower-GPA group has a higher share of students with shorter study time.\n‚Ä¢ Social media time: The lower-GPA group shows a higher median social media time, suggesting they spend more time on social media.\n‚Ä¢ Friends time: Median differences are small; significance indicates only a weak relationship with GPA. Overall, effect sizes are small, meaning differences are modest and driven more by distributional shifts than large between-group gaps."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#student-perceptions-and-learning-environment",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01final.html#student-perceptions-and-learning-environment",
    "title": "Take-home Exercise 1final",
    "section": "6 Student Perceptions and Learning Environment",
    "text": "6 Student Perceptions and Learning Environment\nIn section 5, we explored the relationship and differences between behavior and GPA. Now, let‚Äôs move on to the next section on perception. After completing the previous section, I couldn‚Äôt help but wonder: besides behavior, are there also systemic differences between perceptual experience and GPA? Which perceptual dimensions are most relevant?\n\nViz6: Students are most satisfied with school but personal habits still decides at certain cases\nThis figure uses HH to plot, illustrating the frequency distribution of students‚Äô responses to eight learning perception items on a 1‚Äì5 Likert scale. First, the eight variables (table b) were extracted from final_ggplot, and then convert the wide table to a long table, resulting in two columns: Measure and Response. I then created a contingency table for these two columns, converting each row to a percentage. This was done to eliminate the impact of sample size differences on the graph‚Äôs scale. Finally, I used the HH::likert() function to plot the Diverging Stacked Bar, using percentages as a uniform measure to directly show the differences and frequency distribution of each variable.\n\n\nCode\nsurvey_vars &lt;- c(\"Study_Methods\", \"SupportOf_Uni\", \"SupportOf_Lec\", \"Facilitie_Uni\", \n                 \"Quality_Lecturer\", \"TrainingCurriculum\", \"Competitive_Class\", \"InfuenceF_Friends\")\n#prepare long table\nsurvey_long &lt;- final_ggplot[, survey_vars] %&gt;%\n  gather(measure, response)\n#Converting to factor\nsurvey_long$measure &lt;- as.factor(survey_long$measure)\nsurvey_long$response &lt;- as.factor(survey_long$response)\n# Creating the contingency table\nsurvey_df &lt;- table(survey_long$measure, survey_long$response) %&gt;%\n  as.data.frame.matrix()\n# Changing the column name\ncolnames(survey_df) &lt;- c(\"Not at all\", \"Little\", \"Moderate\", \"Quite\", \"Very\")\n\nrownames(survey_df) &lt;- c(\"Class Competition\", \"University Facilities\", \"Peer Influence\", \n                         \"Lecturer Support\", \"Lecturer Quality\", \"Study Methods\", \n                         \"University Support\", \"Training Curriculum\")\n# Changing the dataframe row name\nsurvey_df &lt;- tibble::rownames_to_column(survey_df, var = \"Measure\")\n# plot using HH package\nlikert(Measure ~ ., \n       data = survey_df, \n       ylab = NULL,\n       ReferenceZero = 3, \n       as.percent = TRUE,\n       positive.order = TRUE, \n       main = list(\" Most of Students are generally satisfied with Learning Perceptions\", x = unit(.55, \"npc\")), \n       sub = list(\"Satisfaction Rating (1-5 Scale)\", x = unit(.57, \"npc\")),\n       xlim = c(-100, -80, -60, -40, -20, 0, 20, 40, 60, 80, 100),\n       strip = FALSE)\n\n\n\n\n\n\n\n\n\n-   ::: {.callout-note icon=\"false\"}\n    ## üò∫ Observations\n\n    -   Overall, the eight variables related to learning perceptions are significantly biased towards the positive: \"very\" generally accounts for a larger proportion than \"not at all,\" indicating respondents have a positive evaluation of the school's environment and support. The proportion of \"very\" is large for teaching-related dimensions, indicating respondents are satisfied with these variables. Conversely, proportions of \"Moderate\" and \"little\" are relatively high for \"Study_Methods\" and \"influence from friends,\" indicating these factors are more likely related to personal habits.\n\n    -   This leads to a conclusion that respondents are generally satisfied with learning perceptions, but some factors still depend on individual differences. Besides, it also provides an overview for subsequent analysis of the relationship between perception variables and GPA.\n    :::\n:::\n\n\nViz7: Does GPA have the relationship with perception variables?\nFirst, I selected all perceptual variables and GPA (num). Since Likert categorizes variables by class, I converted all variables into ordered numerical representations by as.numeric function. Then, I used cor(method=‚Äúspearman‚Äù) to generate the Spearman correlation matrix. Finally, I used the basic function heatmap() to visualize the correlation matrix, preserving the hierarchical clustering dendrogram to observe the similarity and clustering structure between variables.\n\n\nCode\n# selecting vars\nvar_list &lt;- c(\"GPA_Num\", \"Adapt_Learning_Uni\", \"Study_Methods\", \"SupportOf_Uni\", \n              \"SupportOf_Lec\", \"Facilitie_Uni\", \"Quality_Lecturer\", \n              \"TrainingCurriculum\", \"Competitive_Class\", \"InfuenceF_Friends\")\ncorrel_var &lt;- final_ggplot[, var_list]\n# turn into numerical as.numeric\ncorrel_var$GPA_Num           &lt;- as.numeric(as.factor(correl_var$GPA_Num))\ncorrel_var$Adapt_Learning_Uni &lt;- as.numeric(as.factor(correl_var$Adapt_Learning_Uni))\ncorrel_var$Study_Methods     &lt;- as.numeric(as.factor(correl_var$Study_Methods))\ncorrel_var$SupportOf_Uni     &lt;- as.numeric(as.factor(correl_var$SupportOf_Uni))\ncorrel_var$SupportOf_Lec     &lt;- as.numeric(as.factor(correl_var$SupportOf_Lec))\ncorrel_var$Facilitie_Uni     &lt;- as.numeric(as.factor(correl_var$Facilitie_Uni))\ncorrel_var$Quality_Lecturer  &lt;- as.numeric(as.factor(correl_var$Quality_Lecturer))\ncorrel_var$TrainingCurriculum &lt;- as.numeric(as.factor(correl_var$TrainingCurriculum))\ncorrel_var$Competitive_Class &lt;- as.numeric(as.factor(correl_var$Competitive_Class))\ncorrel_var$InfuenceF_Friends &lt;- as.numeric(as.factor(correl_var$InfuenceF_Friends))\n\n# spearman matrix\nwh_matrix &lt;- cor(correl_var, method = \"spearman\")\nwh_matrix[is.na(wh_matrix)] &lt;- 0\n\n# plot heatmap\nheatmap(\n  wh_matrix,\n  Rowv = NA, Colv = NA,         \n  scale = \"none\", \n  cexRow = 0.9, cexCol = 0.9,\n  margins = c(12, 10),\n  main = \"Does GPA have the relationship with perception variables?\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteüò∫ Observations\n\n\n\n\nWe can easily observe significant positive correlations among the nine learning perception variables (darker areas), indicating that these perception dimensions are interdependent and related. For example, ‚Äúschool support/teacher support/curriculum and facilities‚Äù tend to cluster together, suggesting a high degree of consistency in students‚Äô overall evaluations of the learning environment and teachers.\nNotably, the GPA_Num row/column is very light in color, indicating almost no correlation between GPA and the learning perception variables. The overall trend is that the perception variables have a strong synergistic effect, while GPA has virtually no relationship with any single perception variable."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "This file contains chapter 6,14,15,16"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#overview",
    "title": "Hands-on Exercise 5b",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nTo summerize, Correlation measures the direction and strength of a linear relationship between two variables, ranging from -1 (perfect negative) to +1 (perfect positive), with 0 indicating no linear relationship. For multivariate data, correlations are summarized in a correlation (or scatterplot) matrix, which helps (1) reveal pairwise relationships in high-dimensional data, (2) serve as input for models like factor analysis, SEM, and regression, and (3) diagnose issues such as multicollinearity in regression. For large datasets, corrgrams provide an efficient visual way to show correlation sign/magnitude and cluster similar variables. This exercise demonstrates how to visualize correlation matrices in R using pairs(), corrplot, and interactive Plotly."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5b",
    "section": "6.2 Installing and Launching R Packages",
    "text": "6.2 Installing and Launching R Packages\nI open a newed Quarto document with Keeping the default html authoring format. Next, use the code chunk below to install and launch corrplot, ggpubr, plotly and tidyverse in RStudio.\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#importing-and-preparing-the-data-set",
    "title": "Hands-on Exercise 5b",
    "section": "6.3 Importing and Preparing The Data Set",
    "text": "6.3 Importing and Preparing The Data Set\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\n6.3.1 Importing Data\nFirst, let us import the data into R by using read_csv() of readr package.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nRows: 6497 Columns: 13\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (1): type\ndbl (12): fixed acidity, volatile acidity, citric acid, residual sugar, chlo...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHere beside quality and type, the rest of the variables are numerical and continuous data type."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#building-correlation-matrix-pairs-method",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#building-correlation-matrix-pairs-method",
    "title": "Hands-on Exercise 5b",
    "section": "6.4 Building Correlation Matrix: pairs() method",
    "text": "6.4 Building Correlation Matrix: pairs() method\nThere are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\nBefore you continue to the next step, you should read the syntax description of pairsfunction.\n\n6.4.1 Building a basic correlation matrix\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\n\nObservations: The required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\n\n6.4.2 Drawing the lower corner\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\nSimilarly, you can display the upper half of the correlation matrix by using the code chun below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\nObservations: upper.panel function is used for showing half of the correlation matrix.\n\n\n6.4.3 Including with correlation coefficients\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\nDon‚Äôt worry about the details for now-just type this code into your R session or script. Let‚Äôs have more fun way to display the correlation matrix.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 5b",
    "section": "6.5 Visualising Correlation Matrix: ggcormat()",
    "text": "6.5 Visualising Correlation Matrix: ggcormat()\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e.¬†more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram. In this section, you will learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\n\n6.5.1 The basic plot\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it‚Äôs ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine,\n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#building-multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#building-multiple-plots",
    "title": "Hands-on Exercise 5b",
    "section": "6.6 Building multiple plots",
    "text": "6.6 Building multiple plots\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 0.4),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\nThings to learn:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to [patchwork::wrap_plots]https://patchwork.data-imaginist.com/reference/wrap_plots.html, except for guides argument which is already separately specified earlier.The &lt;plot&gt; function wrap_plots makes it easy to add a list of plots and layout specifications to a single plot.\nLikewise, annotation.args argument is calling plot [annotation]https://patchwork.data-imaginist.com/reference/plot_annotation.html arguments of patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on Exercise 5b",
    "section": "6.7 Visualising Correlation Matrix using corrplot Package",
    "text": "6.7 Visualising Correlation Matrix using corrplot Package\nIn this hands-on exercise, we will focus on corrplot. However, you are encouraged to explore the other two packages too.\nBefore getting started, you are required to read [An Introduction to corrplot Package]https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html in order to gain basic understanding of corrplot package.\n\n6.7.1 Getting started with corrplot\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame. In the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n6.7.2 Working with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\nFeel free to change the method argument to other supported visual geometrics, In corrplot, besides ‚Äúellipse‚Äù, these are some commonly used methods (all officially supported): ‚Äúcircle‚Äù: The size/color of the circle represents the correlation strength. ‚Äúsquare‚Äù: The size/color of the square represents the correlation strength. ‚Äúnumber‚Äù: Directly displays the correlation coefficient number. ‚Äúshade‚Äù: Uses shades of shadow to represent strength.\n‚Äúcolor‚Äù: Solid color block (heatmap style).\n‚Äúpie‚Äù: Small pie chart sector representing strength.\n‚Äúblank‚Äù: Blank (often used to display only the upper/lower triangle, leaving the other half empty).\n\n\n6.7.3 Working with layout\ncorrplor() supports three layout types, namely: ‚Äúfull‚Äù, ‚Äúupper‚Äù or ‚Äúlower‚Äù. The default is ‚Äúfull‚Äù which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\nPlease feel free to experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset, just to mention a few of them.\n\n\n6.7.4 Working with mixed layout\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe code chunk used to plot the corrgram are shown below.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nObservations:Notice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e.¬†number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n6.7.5 Combining corrgram with the significant test\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid. With corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n6.7.6 Reorder a corrgram\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e.¬†‚Äúoriginal‚Äù). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n‚ÄúAOE‚Äù is for the angular order of the eigenvectors. See Michael Friendly (2002) for details. ‚ÄúFPC‚Äù for the first principal component order. ‚Äúhclust‚Äù for hierarchical clustering order, and ‚Äúhclust.method‚Äù for the agglomeration method to be used. ‚Äúhclust.method‚Äù should be one of ‚Äúward‚Äù, ‚Äúsingle‚Äù, ‚Äúcomplete‚Äù, ‚Äúaverage‚Äù, ‚Äúmcquitty‚Äù, ‚Äúmedian‚Äù or ‚Äúcentroid‚Äù. ‚Äúalphabet‚Äù for alphabetical order. ‚ÄúAOE‚Äù, ‚ÄúFPC‚Äù, ‚Äúhclust‚Äù, ‚Äúalphabet‚Äù. More algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n6.7.7 Reordering a correlation matrix using hclust\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#reference",
    "title": "Hands-on Exercise 5b",
    "section": "6.7 Reference",
    "text": "6.7 Reference\nMichael Friendly (2002). ‚ÄúCorrgrams: Exploratory displays for correlation matrices‚Äù. The American Statistician, 56, 316‚Äì324.\nD.J. Murdoch, E.D. Chow (1996). ‚ÄúA graphical display of large correlation matrices‚Äù. The American Statistician, 50, 178‚Äì180.\n7.1 R packages\n\nggcormat() of ggstatsplot package\nggscatmat and ggpairs of GGally.\ncorrplot. A graphical display of a correlation matrix or general matrix. It also contains some algorithms to do matrix reordering. In addition, corrplot is good at details, including choosing color, text labels, color labels, layout, etc.\ncorrgram calculates correlation of variables and displays the results graphically. Included panel functions can display points, shading, ellipses, and correlation values with confidence intervals."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#overview-1",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#overview-1",
    "title": "Hands-on Exercise 5b",
    "section": "14.1 Overview",
    "text": "14.1 Overview\nHeatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#installing-and-launching-r-packages-1",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#installing-and-launching-r-packages-1",
    "title": "Hands-on Exercise 5b",
    "section": "14.2 Installing and Launching R Packages",
    "text": "14.2 Installing and Launching R Packages\nBefore you get started, you are required to open a new Quarto document. Keep the default html as the authoring format.\nNext, you will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#importing-and-preparing-the-data-set-1",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#importing-and-preparing-the-data-set-1",
    "title": "Hands-on Exercise 5b",
    "section": "14.3 Importing and Preparing The Data Set",
    "text": "14.3 Importing and Preparing The Data Set\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n14.3.1 Importing the data set\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nRows: 156 Columns: 12\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (2): Country, Region\ndbl (10): Happiness score, Whisker-high, Whisker-low, Dystopia, GDP per capi...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe output tibbled data frame is called wh.\n\n\n14.3.2 Preparing the data\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n14.3.3 Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format.\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n\n14.4.1 heatmap() of R Stats\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap &lt;- heatmap(wh_matrix, Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#creating-interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#creating-interactive-heatmap",
    "title": "Hands-on Exercise 5b",
    "section": "14.5 Creating Interactive Heatmap",
    "text": "14.5 Creating Interactive Heatmap\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manualof the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n14.5.1 Working with heatmaply\n\nheatmaply(mtcars)\n\nWarning in doTryCatch(return(expr), name, parentenv, handler): unable to load shared object '/Library/Frameworks/R.framework/Resources/modules//R_X11.so':\n  dlopen(/Library/Frameworks/R.framework/Resources/modules//R_X11.so, 0x0006): Library not loaded: /opt/X11/lib/libSM.6.dylib\n  Referenced from: &lt;7ECC4104-EC6A-38FD-9BEA-BFE0B870925C&gt; /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/modules/R_X11.so\n  Reason: tried: '/opt/X11/lib/libSM.6.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/X11/lib/libSM.6.dylib' (no such file), '/opt/X11/lib/libSM.6.dylib' (no such file), '/Library/Frameworks/R.framework/Resources/lib/libSM.6.dylib' (no such file), '/Library/Java/JavaVirtualMachines/jdk-11.0.18+10/Contents/Home/lib/server/libSM.6.dylib' (no such file)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n‚Ñπ The deprecated feature was likely used in the dendextend package.\n  Please report the issue at &lt;https://github.com/talgalili/dendextend/issues&gt;.\n\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n14.5.2 Data trasformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables‚Äô values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n14.5.2.1 Scaling method\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n14.5.2.2 Normalising method\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable‚Äôs distribution while making them easily comparable on the same ‚Äúscale‚Äù.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e.¬†wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n14.5.2.3 Percentising method\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e.¬†wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n14.5.3 Clustering algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options ‚Äúpearson‚Äù, ‚Äúspearman‚Äù and ‚Äúkendall‚Äù can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in ‚Äúeuclidean‚Äù to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is ‚Äúdist‚Äù‚Äù hence this can be one of ‚Äúeuclidean‚Äù, ‚Äúmaximum‚Äù, ‚Äúmanhattan‚Äù, ‚Äúcanberra‚Äù, ‚Äúbinary‚Äù or ‚Äúminkowski‚Äù.\nhclust_method default is NULL, which results in ‚Äúcomplete‚Äù method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of ‚Äúward.D‚Äù, ‚Äúward.D2‚Äù, ‚Äúsingle‚Äù, ‚Äúcomplete‚Äù, ‚Äúaverage‚Äù (= UPGMA), ‚Äúmcquitty‚Äù (= WPGMA), ‚Äúmedian‚Äù (= WPGMC) or ‚Äúcentroid‚Äù (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n14.5.4 Manual approach\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with ‚ÄúEuclidean distance‚Äù and ‚Äúward.D‚Äù method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n14.5.5 Statistical approach\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that ‚Äúaverage‚Äù method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n14.5.6 Seriation\nOne of the problems with hierarchical clustering is that it doesn‚Äôt actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can‚Äôt end up between A and B, but it doesn‚Äôt tell you which way to flip the A+B cluster. It doesn‚Äôt tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is ‚ÄúOLO‚Äù (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is ‚ÄúGW‚Äù (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\nRegistered S3 method overwritten by 'gclus':\n  method         from     \n  reorder.hclust seriation\n\n\n\n\n\n\nThe option ‚Äúmean‚Äù gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option ‚Äúnone‚Äù gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n14.5.7 Working with colour palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n14.5.8 The finishing touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 2.5,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#overview-2",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#overview-2",
    "title": "Hands-on Exercise 5b",
    "section": "15.1 Overview",
    "text": "15.1 Overview\nParallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), ‚ÄúThis certainly isn‚Äôt a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn‚Äôt in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.‚Äù For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#installing-and-launching-r-packages-2",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#installing-and-launching-r-packages-2",
    "title": "Hands-on Exercise 5b",
    "section": "15.2 Installing and Launching R Packages",
    "text": "15.2 Installing and Launching R Packages\nFor this exercise, the GGally, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#data-preparation",
    "title": "Hands-on Exercise 5b",
    "section": "15.3 Data Preparation",
    "text": "15.3 Data Preparation\nIn this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nRows: 156 Columns: 12\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (2): Country, Region\ndbl (10): Happiness score, Whisker-high, Whisker-low, Dystopia, GDP per capi...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on Exercise 5b",
    "section": "15.4 Plotting Static Parallel Coordinates Plot",
    "text": "15.4 Plotting Static Parallel Coordinates Plot\nIn this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n15.4.1 Plotting a simple parallel coordinates\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))+\n  theme(\n    axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 7),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    plot.margin  = margin(t = 10, r = 10, b = 50, l = 10)\n  )\n\n\n\n\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e.¬†wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n15.4.2 Plotting a parallel coordinates with boxplot\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")+\n  theme(\n    axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 7),\n    axis.title.x = element_blank(),              \n    plot.title = element_text(margin = margin(b = 10)),\n    plot.margin = margin(t = 10, r = 10, b = 70, l = 10)  \n  )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e.¬†parallel lines) by using a single variable (i.e.¬†Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\n\n\n\n15.4.3 Parallel coordinates with facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n15.4.4 Rotating x-axis text label\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\n15.4.5 Adjusting the rotated x-axis text label\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme‚Äôs text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 100, hjust=0.1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Exercise 5b",
    "section": "15.5 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods",
    "text": "15.5 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‚Äòhtmlwidgets‚Äô package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n15.5.1 The basic plot\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\n15.5.2 Rotate axis label\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n15.5.3 Changing the colour scheme\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n15.5.4 Parallel coordinates plot with histogram\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#references",
    "title": "Hands-on Exercise 5b",
    "section": "15.6 References",
    "text": "15.6 References\n\nggparcoord() of GGally package\nparallelPlot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#overview-3",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#overview-3",
    "title": "Hands-on Exercise 5b",
    "section": "16.1 Overview",
    "text": "16.1 Overview\nIn this hands-on exercise, you will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#installing-and-launching-r-packages-3",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#installing-and-launching-r-packages-3",
    "title": "Hands-on Exercise 5b",
    "section": "16.2 Installing and Launching R Packages",
    "text": "16.2 Installing and Launching R Packages\nBefore we get started, you are required to check if treemap and tidyverse pacakges have been installed in you R.\n\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#data-wrangling",
    "title": "Hands-on Exercise 5b",
    "section": "16.3 Data Wrangling",
    "text": "16.3 Data Wrangling\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\n16.3.1 Importing the data set\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nRows: 23205 Columns: 20\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (12): Project Name, Address, Type of Area, Nett Price($), Sale Date, Pro...\ndbl  (8): No. of Units, Area (sqm), Transacted Price ($), Unit Price ($ psm)...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe output tibble data.frame is called realis2018.\n\n\n16.3.2 Data Wrangling and Manipulation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No.¬†of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they‚Äôll be automatically applied ‚Äúby group‚Äù.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(‚Äúwindow-functions‚Äù).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n16.3.3 Grouped summaries without the Pipe\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n`summarise()` has grouped output by 'Project Name', 'Planning Region',\n'Planning Area', 'Property Type'. You can override using the `.groups`\nargument.\n\n\nNotes:Aggregation functions such as sum() and meadian() obey the usual rule of missing values: if there‚Äôs any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don‚Äôt have to care about it.\n\n\n16.3.4 Grouped summaries with the pipe\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%: (To learn more about pipe, visit this excellent article: Pipes in R Tutorial For Beginners.)\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n`summarise()` has grouped output by 'Project Name', 'Planning Region',\n'Planning Area', 'Property Type'. You can override using the `.groups`\nargument."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#designing-treemap-with-treemap-package",
    "title": "Hands-on Exercise 5b",
    "section": "16.4 Designing Treemap with treemap Package",
    "text": "16.4 Designing Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n16.4.1 Designing a static treemap\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n16.4.2 Using the basic arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it‚Äôs vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n16.4.3 Working with vColor and type arguments\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThinking to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e.¬†0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n16.4.4 Colours in treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between ‚Äúvalue‚Äù and ‚Äúmanual‚Äù is the default value for mapping. The ‚Äúvalue‚Äù treemap considers palette to be a diverging color palette (say ColorBrewer‚Äôs ‚ÄúRdYlBu‚Äù), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The ‚Äúmanual‚Äù treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n16.4.5 The ‚Äúvalue‚Äù type treemap\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n16.4.6 The ‚Äúmanual‚Äù type treemap\nThe ‚Äúmanual‚Äù type does not interpret the values as the ‚Äúvalue‚Äù type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n16.4.7 Treemap Layout\ntreemap() supports two popular treemap layouts, namely: ‚Äúsquarified‚Äù and ‚ÄúpivotSize‚Äù. The default is ‚ÄúpivotSize‚Äù.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n16.4.8 Working with algorithm argument\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n16.4.9 Using sortID\nWhen ‚ÄúpivotSize‚Äù algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Exercise 5b",
    "section": "16.5 Designing Treemap using treemapify Package",
    "text": "16.5 Designing Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to ‚Äútreemapify‚Äù its user guide.\n\n16.5.1 Designing a basic treemap\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\nWarning in fortify(data, ...): Arguments in `...` must be used.\n‚úñ Problematic arguments:\n‚Ä¢ layout = \"scol\"\n‚Ä¢ start = \"bottomleft\"\n‚Ñπ Did you misspell an argument name?\n\n\n\n\n\n\n\n\n\n\n\n16.5.2 Defining hierarchy\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\nWarning in fortify(data, ...): Arguments in `...` must be used.\n‚úñ Problematic argument:\n‚Ä¢ start = \"topleft\"\n‚Ñπ Did you misspell an argument name?\n\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex05b/Hands-on_Ex05b.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Exercise 5b",
    "section": "16.6 Designing Interactive Treemap using d3treeR",
    "text": "16.6 Designing Interactive Treemap using d3treeR\n\n16.6.1 Installing d3treeR package\nThis slide shows you how to install a R package which is not available in cran.\n\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\n\ninstall.packages(\"devtools\")\n\n2.Next, you will load the devtools library and install the package found in github by using the codes below.\n\nlibrary(devtools)\n\nLoading required package: usethis\n\ninstall_github(\"timelyportfolio/d3treeR\")\n\nUsing GitHub PAT from the git credential store.\n\n\nSkipping install of 'd3treeR' from a github remote, the SHA1 (ebb833db) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\n3.Now you are ready to launch d3treeR package\n\nlibrary(d3treeR)\n\n\n\n16.6.2 Designing An Interactive Treemap\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on Exercise 5",
    "section": "",
    "text": "*numbers stands for the chapter, i.e 13 stands for chapter 13. This file contains chapter 13 only, others are in the Hands-on_Ex05b file."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "title": "Hands-on Exercise 5",
    "section": "13.1 Overview",
    "text": "13.1 Overview\nTernary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It‚Äôs display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5",
    "section": "13.2 Installing and launching R packages",
    "text": "13.2 Installing and launching R packages\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly‚Äôs JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nThe code chunks below will accomplish the task.\n\npacman::p_load(plotly, ggtern, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-preparation",
    "title": "Hands-on Exercise 5",
    "section": "13.3 Data Preparation",
    "text": "13.3 Data Preparation\n\n13.3.1 The data\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n13.3.2 Importing Data\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\nRows: 108126 Columns: 5\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (3): PA, SZ, AG\ndbl (2): Year, Population\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe result shows the dataset has 108126 rows and 5 columns.\n\n\n13.3.3 Preparing the Data\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Exercise 5",
    "section": "13.4 Plotting Ternary Diagram with R",
    "text": "13.4 Plotting Ternary Diagram with R\n\n13.4.1 4.1 Plotting a static ternary diagram\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\nObservations:the second plot is better than the first one because it has arrows, color and detailed title, which considers more desirable than first one.\n\n\n13.4.2 Plotting an interative ternary diagram\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )\n\nNo scatterternary mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\nObservations: the code for this interactive ternary plot can be reused and annotations is very eye-catching."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "This exercise we come to a new section, which is Visualising and Analysing Time-series Data,and first we should go to chapter 17.Let‚Äôs start\n#17 Visualising and Analysing Time-oriented Data ##17.1 Learning Outcome In this exercise, we will know how to plot by using r package, they are:a calender heatmap,cycle plot,slopegraph,a horizon chart ##17.2 Getting Started\n\npacman::p_load(scales, viridis, lubridate, ggthemes,\n               gridExtra, readxl, knitr, data.table,\n               CGPfunctions, ggHoriPlot, tidyverse)\n\nWarning: package 'CGPfunctions' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\nWarning: 'BiocManager' not available.  Could not check Bioconductor.\n\nPlease use `install.packages('BiocManager')` and then retry.\n\n\nWarning in p_install(package, character.only = TRUE, ...):\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'CGPfunctions'\n\n\nWarning in pacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, : Failed to install/load:\nCGPfunctions\n\n\n##17.3 Plotting Calendar Heatmap In this section, we will learn how to plot a calender heatmap programmatically by using ggplot2 package. ##17.4 The data eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country. ###17.4.2 Importing Data First, you will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\nRows: 199999 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (2): source_country, tz\ndttm (1): timestamp\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n##1#7.4.3 Examining the data structure It is always a good practice to examine the imported data frame before further analysis is performed. For example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nObservations:\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n17.4.4 Data Preparation\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task."
  }
]